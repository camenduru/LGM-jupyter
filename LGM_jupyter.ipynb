{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/LGM-jupyter/blob/main/LGM_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone --recursive -b dev https://github.com/camenduru/LGM-hf\n",
        "%cd /content/LGM-hf\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ashawkey/LGM/resolve/main/model.safetensors -d /content/LGM-hf/model -o model.safetensors\n",
        "\n",
        "!pip install -q tyro diffusers dearpygui einops accelerate lpips pygltflib rembg[gpu,cli] trimesh kiui xatlas roma plyfile\n",
        "!pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "!pip install -q ./diff-gaussian-rasterization\n",
        "\n",
        "import os\n",
        "import tyro\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from safetensors.torch import load_file\n",
        "import rembg\n",
        "\n",
        "ckpt_path = \"/content/LGM-hf/model/model.safetensors\"\n",
        "\n",
        "import kiui\n",
        "from kiui.op import recenter\n",
        "from kiui.cam import orbit_camera\n",
        "\n",
        "from core.options import AllConfigs, Options\n",
        "from core.models import LGM\n",
        "from mvdream.pipeline_mvdream import MVDreamPipeline\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "GRADIO_VIDEO_PATH = 'gradio_output.mp4'\n",
        "GRADIO_PLY_PATH = 'gradio_output.ply'\n",
        "\n",
        "# opt = tyro.cli(AllConfigs)\n",
        "opt = Options(\n",
        "    input_size=256,\n",
        "    up_channels=(1024, 1024, 512, 256, 128), # one more decoder\n",
        "    up_attention=(True, True, True, False, False),\n",
        "    splat_size=128,\n",
        "    output_size=512, # render & supervise Gaussians at a higher resolution.\n",
        "    batch_size=8,\n",
        "    num_views=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    mixed_precision='bf16',\n",
        "    resume=ckpt_path,\n",
        ")\n",
        "\n",
        "# model\n",
        "model = LGM(opt)\n",
        "\n",
        "# resume pretrained checkpoint\n",
        "if opt.resume is not None:\n",
        "    if opt.resume.endswith('safetensors'):\n",
        "        ckpt = load_file(opt.resume, device='cpu')\n",
        "    else:\n",
        "        ckpt = torch.load(opt.resume, map_location='cpu')\n",
        "    model.load_state_dict(ckpt, strict=False)\n",
        "    print(f'[INFO] Loaded checkpoint from {opt.resume}')\n",
        "else:\n",
        "    print(f'[WARN] model randomly initialized, are you sure?')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.half().to(device)\n",
        "model.eval()\n",
        "\n",
        "tan_half_fov = np.tan(0.5 * np.deg2rad(opt.fovy))\n",
        "proj_matrix = torch.zeros(4, 4, dtype=torch.float32, device=device)\n",
        "proj_matrix[0, 0] = 1 / tan_half_fov\n",
        "proj_matrix[1, 1] = 1 / tan_half_fov\n",
        "proj_matrix[2, 2] = (opt.zfar + opt.znear) / (opt.zfar - opt.znear)\n",
        "proj_matrix[3, 2] = - (opt.zfar * opt.znear) / (opt.zfar - opt.znear)\n",
        "proj_matrix[2, 3] = 1\n",
        "\n",
        "# load dreams\n",
        "pipe_text = MVDreamPipeline.from_pretrained(\n",
        "    'ashawkey/mvdream-sd2.1-diffusers', # remote weights\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    # local_files_only=True,\n",
        ")\n",
        "pipe_text = pipe_text.to(device)\n",
        "\n",
        "pipe_image = MVDreamPipeline.from_pretrained(\n",
        "    \"ashawkey/imagedream-ipmv-diffusers\", # remote weights\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    # local_files_only=True,\n",
        ")\n",
        "pipe_image = pipe_image.to(device)\n",
        "\n",
        "# load rembg\n",
        "bg_remover = rembg.new_session()\n",
        "\n",
        "# process function\n",
        "def process(input_image, prompt, prompt_neg='', input_elevation=0, input_num_steps=30, input_seed=42):\n",
        "\n",
        "    # seed\n",
        "    kiui.seed_everything(input_seed)\n",
        "\n",
        "    os.makedirs(opt.workspace, exist_ok=True)\n",
        "    output_video_path = os.path.join(opt.workspace, GRADIO_VIDEO_PATH)\n",
        "    output_ply_path = os.path.join(opt.workspace, GRADIO_PLY_PATH)\n",
        "\n",
        "    # text-conditioned\n",
        "    if input_image is None:\n",
        "        mv_image_uint8 = pipe_text(prompt, negative_prompt=prompt_neg, num_inference_steps=input_num_steps, guidance_scale=7.5, elevation=input_elevation)\n",
        "        mv_image_uint8 = (mv_image_uint8 * 255).astype(np.uint8)\n",
        "        # bg removal\n",
        "        mv_image = []\n",
        "        for i in range(4):\n",
        "            image = rembg.remove(mv_image_uint8[i], session=bg_remover) # [H, W, 4]\n",
        "            # to white bg\n",
        "            image = image.astype(np.float32) / 255\n",
        "            image = recenter(image, image[..., 0] > 0, border_ratio=0.2)\n",
        "            image = image[..., :3] * image[..., -1:] + (1 - image[..., -1:])\n",
        "            mv_image.append(image)\n",
        "    # image-conditioned (may also input text, but no text usually works too)\n",
        "    else:\n",
        "        input_image = np.array(input_image) # uint8\n",
        "        # bg removal\n",
        "        carved_image = rembg.remove(input_image, session=bg_remover) # [H, W, 4]\n",
        "        mask = carved_image[..., -1] > 0\n",
        "        image = recenter(carved_image, mask, border_ratio=0.2)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = image[..., :3] * image[..., 3:4] + (1 - image[..., 3:4])\n",
        "        mv_image = pipe_image(prompt, image, negative_prompt=prompt_neg, num_inference_steps=input_num_steps, guidance_scale=5.0,  elevation=input_elevation)\n",
        "        \n",
        "    mv_image_grid = np.concatenate([\n",
        "        np.concatenate([mv_image[1], mv_image[2]], axis=1),\n",
        "        np.concatenate([mv_image[3], mv_image[0]], axis=1),\n",
        "    ], axis=0)\n",
        "\n",
        "    # generate gaussians\n",
        "    input_image = np.stack([mv_image[1], mv_image[2], mv_image[3], mv_image[0]], axis=0) # [4, 256, 256, 3], float32\n",
        "    input_image = torch.from_numpy(input_image).permute(0, 3, 1, 2).float().to(device) # [4, 3, 256, 256]\n",
        "    input_image = F.interpolate(input_image, size=(opt.input_size, opt.input_size), mode='bilinear', align_corners=False)\n",
        "    input_image = TF.normalize(input_image, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
        "\n",
        "    rays_embeddings = model.prepare_default_rays(device, elevation=input_elevation)\n",
        "    input_image = torch.cat([input_image, rays_embeddings], dim=1).unsqueeze(0) # [1, 4, 9, H, W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            # generate gaussians\n",
        "            gaussians = model.forward_gaussians(input_image)\n",
        "        \n",
        "        # save gaussians\n",
        "        model.gs.save_ply(gaussians, output_ply_path)\n",
        "        \n",
        "        # render 360 video \n",
        "        images = []\n",
        "        elevation = 0\n",
        "        if opt.fancy_video:\n",
        "            azimuth = np.arange(0, 720, 4, dtype=np.int32)\n",
        "            for azi in tqdm.tqdm(azimuth):\n",
        "                \n",
        "                cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
        "\n",
        "                cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
        "                \n",
        "                # cameras needed by gaussian rasterizer\n",
        "                cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
        "                cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
        "                cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
        "\n",
        "                scale = min(azi / 360, 1)\n",
        "\n",
        "                image = model.gs.render(gaussians, cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=scale)['image']\n",
        "                images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
        "        else:\n",
        "            azimuth = np.arange(0, 360, 2, dtype=np.int32)\n",
        "            for azi in tqdm.tqdm(azimuth):\n",
        "                \n",
        "                cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
        "\n",
        "                cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
        "                \n",
        "                # cameras needed by gaussian rasterizer\n",
        "                cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
        "                cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
        "                cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
        "\n",
        "                image = model.gs.render(gaussians, cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=1)['image']\n",
        "                images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
        "\n",
        "        images = np.concatenate(images, axis=0)\n",
        "        imageio.mimwrite(output_video_path, images, fps=30)\n",
        "\n",
        "    return mv_image_grid, output_video_path, output_ply_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process(None, prompt='a frog', prompt_neg='ugly, blurry, pixelated obscure, unnatural colors, poor lighting, dull, unclear, cropped, lowres, low quality, artifacts, duplicate', input_elevation=0, input_num_steps=30, input_seed=42)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
